{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\ricardo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\ricardo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imblearn) (0.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\ricardo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\ricardo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\ricardo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\ricardo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from imbalanced-learn->imblearn) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ricardo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (3.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "# display plots in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns\n",
    "### sns.set_style('darkgrid')\n",
    "\n",
    "# store elements as dictionary keys and their counts as dictionary values\n",
    "from collections import Counter\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Classification metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Function for creating model pipelines - sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Function for creating model pipelines - imblearn\n",
    "from imblearn.pipeline import make_pipeline as imbl_pipe\n",
    "\n",
    "# Over-sampling using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CreditScore Geography  Gender  Age  Tenure    Balance  NumOfProducts  \\\n",
       "0          619    France  Female   42       2       0.00              1   \n",
       "1          608     Spain  Female   41       1   83807.86              1   \n",
       "2          502    France  Female   42       8  159660.80              3   \n",
       "3          699    France  Female   39       1       0.00              2   \n",
       "4          850     Spain  Female   43       2  125510.82              1   \n",
       "\n",
       "   HasCrCard  IsActiveMember  EstimatedSalary  Exited  \n",
       "0          1               1        101348.88       1  \n",
       "1          0               1        112542.58       0  \n",
       "2          1               0        113931.57       1  \n",
       "3          0               0         93826.63       0  \n",
       "4          1               1         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abt = pd.read_csv(\"../Resources/analytical_base_table.csv\")\n",
    "abt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start by splitting our dataframe into separate objects:\n",
    "\n",
    "y for the target varibale\n",
    "\n",
    "X for the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 9) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Separate dataframe into separate object\n",
    "\n",
    "# Object for target variable\n",
    "y = abt.Exited\n",
    "\n",
    "# object for input features\n",
    "X = abt.drop(['Exited','Geography'], axis=1)\n",
    "\n",
    "# display shapes of X and y\n",
    "print(X.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CreditScore',\n",
       " 'Age',\n",
       " 'Tenure',\n",
       " 'Balance',\n",
       " 'NumOfProducts',\n",
       " 'HasCrCard',\n",
       " 'IsActiveMember',\n",
       " 'EstimatedSalary']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List numerical features\n",
    "num_columns = X.select_dtypes(include='number').columns.tolist()\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gender']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List categorical features\n",
    "cat_columns = X.select_dtypes(include='object').columns.tolist()\n",
    "cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_count(a):\n",
    "    counter=Counter(a)\n",
    "    kv=[list(counter.keys()),list(counter.values())]\n",
    "    abt2 = pd.DataFrame(np.array(kv).T, columns=['Exited','Count'])\n",
    "    abt2['Count'] = abt2['Count'].astype('int64')\n",
    "    abt2['%'] = round(abt2['Count'] / a.shape[0] * 100, 2)\n",
    "    return abt2.sort_values('Count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exited</th>\n",
       "      <th>Count</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>7963</td>\n",
       "      <td>79.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2037</td>\n",
       "      <td>20.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Exited  Count      %\n",
       "1       0   7963  79.63\n",
       "0       1   2037  20.37"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Train Test Split\n",
    "\n",
    "We will continue with splitting our data into separate training and test sets.\n",
    "\n",
    "30% of observations will be set aside for the test set\n",
    "\n",
    "the rest, 70%, will be used as the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 3000 7000 3000\n"
     ]
    }
   ],
   "source": [
    "random_state = 10\n",
    "\n",
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=random_state,\n",
    "                                                    stratify=abt.Exited)\n",
    "\n",
    "# Print number of observations in X_train, X_test, y_train, and y_test\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7000 entries, 8061 to 4741\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   CreditScore      7000 non-null   int64  \n",
      " 1   Gender           7000 non-null   object \n",
      " 2   Age              7000 non-null   int64  \n",
      " 3   Tenure           7000 non-null   int64  \n",
      " 4   Balance          7000 non-null   float64\n",
      " 5   NumOfProducts    7000 non-null   int64  \n",
      " 6   HasCrCard        7000 non-null   int64  \n",
      " 7   IsActiveMember   7000 non-null   int64  \n",
      " 8   EstimatedSalary  7000 non-null   float64\n",
      "dtypes: float64(2), int64(6), object(1)\n",
      "memory usage: 546.9+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Pipeline\n",
    " \n",
    " ##Scale numerical data and encode categorical data\n",
    "Construct a pre-processing pipeline from the given transformers: MinMaxScaler and OneHotEncoder Create lists of indexes from the list of column namesNeed to be numeric not string to specify columns name in column transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "num_features = [] \n",
    "\n",
    "for i in num_columns:\n",
    "    location = X.columns.get_loc(i)\n",
    "    num_features.append(location)\n",
    "print(num_features)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "cat_features = []\n",
    "\n",
    "for i in cat_columns:\n",
    "    location = X.columns.get_loc(i)\n",
    "    cat_features.append(location)\n",
    "print(cat_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('minmaxscaler', MinMaxScaler(),\n",
       "                                 [0, 2, 3, 4, 5, 6, 7, 8]),\n",
       "                                ('onehotencoder', OneHotEncoder(sparse=False),\n",
       "                                 [1])])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define column transformer\n",
    "# Need to be numeric not string to specify columns name \n",
    "preprocess = make_column_transformer(\n",
    "    (MinMaxScaler(), num_features),\n",
    "    (OneHotEncoder(sparse=False), cat_features)\n",
    ")\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('minmaxscaler',\n",
       "                                                  MinMaxScaler(),\n",
       "                                                  [0, 2, 3, 4, 5, 6, 7, 8]),\n",
       "                                                 ('onehotencoder',\n",
       "                                                  OneHotEncoder(sparse=False),\n",
       "                                                  [1])])),\n",
       "                ('smote', SMOTE(random_state=10)),\n",
       "                ('logisticregression', LogisticRegression(random_state=10))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import classifier\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "# Define model with pipeline\n",
    "model = imbl_pipe(preprocess,\n",
    "                  SMOTE(sampling_strategy='auto', random_state=random_state),\n",
    "                  LogisticRegression(random_state=random_state))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create the GridSearchCV model\n",
    "# Create the GridSearch estimator along with a parameter object containing the values to adjust\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lr_param_grid = {\n",
    "    'logisticregression__C' : [0.01, 0.05, 0.1, 0.5, 1, 5],\n",
    "    'logisticregression__solver' : ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(model, lr_param_grid, verbose=3, cv= 5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "[CV 1/5] END logisticregression__C=0.01, logisticregression__solver=liblinear;, score=0.646 total time=   0.2s\n",
      "[CV 2/5] END logisticregression__C=0.01, logisticregression__solver=liblinear;, score=0.649 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.01, logisticregression__solver=liblinear;, score=0.651 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.01, logisticregression__solver=liblinear;, score=0.639 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.01, logisticregression__solver=liblinear;, score=0.672 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.01, logisticregression__solver=newton-cg;, score=0.648 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.01, logisticregression__solver=newton-cg;, score=0.649 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.01, logisticregression__solver=newton-cg;, score=0.654 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.01, logisticregression__solver=newton-cg;, score=0.645 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.01, logisticregression__solver=newton-cg;, score=0.675 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.01, logisticregression__solver=lbfgs;, score=0.648 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.01, logisticregression__solver=lbfgs;, score=0.649 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.01, logisticregression__solver=lbfgs;, score=0.654 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.01, logisticregression__solver=lbfgs;, score=0.645 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.01, logisticregression__solver=lbfgs;, score=0.674 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.01, logisticregression__solver=sag;, score=0.648 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.01, logisticregression__solver=sag;, score=0.649 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.01, logisticregression__solver=sag;, score=0.655 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.01, logisticregression__solver=sag;, score=0.645 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.01, logisticregression__solver=sag;, score=0.675 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.01, logisticregression__solver=saga;, score=0.648 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.01, logisticregression__solver=saga;, score=0.649 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.01, logisticregression__solver=saga;, score=0.654 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.01, logisticregression__solver=saga;, score=0.645 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.01, logisticregression__solver=saga;, score=0.675 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.05, logisticregression__solver=liblinear;, score=0.677 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.05, logisticregression__solver=liblinear;, score=0.688 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.05, logisticregression__solver=liblinear;, score=0.692 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.05, logisticregression__solver=liblinear;, score=0.679 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.05, logisticregression__solver=liblinear;, score=0.704 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.05, logisticregression__solver=newton-cg;, score=0.680 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.05, logisticregression__solver=newton-cg;, score=0.689 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.05, logisticregression__solver=newton-cg;, score=0.696 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.05, logisticregression__solver=newton-cg;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.05, logisticregression__solver=newton-cg;, score=0.705 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.05, logisticregression__solver=lbfgs;, score=0.680 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.05, logisticregression__solver=lbfgs;, score=0.689 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.05, logisticregression__solver=lbfgs;, score=0.696 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.05, logisticregression__solver=lbfgs;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.05, logisticregression__solver=lbfgs;, score=0.705 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.05, logisticregression__solver=sag;, score=0.680 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.05, logisticregression__solver=sag;, score=0.688 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.05, logisticregression__solver=sag;, score=0.696 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.05, logisticregression__solver=sag;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.05, logisticregression__solver=sag;, score=0.705 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.05, logisticregression__solver=saga;, score=0.680 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.05, logisticregression__solver=saga;, score=0.689 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.05, logisticregression__solver=saga;, score=0.696 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.05, logisticregression__solver=saga;, score=0.683 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.05, logisticregression__solver=saga;, score=0.705 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.1, logisticregression__solver=liblinear;, score=0.693 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.1, logisticregression__solver=liblinear;, score=0.697 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.1, logisticregression__solver=liblinear;, score=0.701 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.1, logisticregression__solver=liblinear;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.1, logisticregression__solver=liblinear;, score=0.714 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.1, logisticregression__solver=newton-cg;, score=0.697 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.1, logisticregression__solver=newton-cg;, score=0.695 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.1, logisticregression__solver=newton-cg;, score=0.703 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.1, logisticregression__solver=newton-cg;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.1, logisticregression__solver=newton-cg;, score=0.717 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.1, logisticregression__solver=lbfgs;, score=0.697 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.1, logisticregression__solver=lbfgs;, score=0.695 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.1, logisticregression__solver=lbfgs;, score=0.703 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.1, logisticregression__solver=lbfgs;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.1, logisticregression__solver=lbfgs;, score=0.717 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.1, logisticregression__solver=sag;, score=0.697 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.1, logisticregression__solver=sag;, score=0.695 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.1, logisticregression__solver=sag;, score=0.703 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.1, logisticregression__solver=sag;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.1, logisticregression__solver=sag;, score=0.717 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.1, logisticregression__solver=saga;, score=0.697 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.1, logisticregression__solver=saga;, score=0.695 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.1, logisticregression__solver=saga;, score=0.703 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.1, logisticregression__solver=saga;, score=0.684 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.1, logisticregression__solver=saga;, score=0.717 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.5, logisticregression__solver=liblinear;, score=0.702 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.5, logisticregression__solver=liblinear;, score=0.702 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.5, logisticregression__solver=liblinear;, score=0.711 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.5, logisticregression__solver=liblinear;, score=0.690 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.5, logisticregression__solver=liblinear;, score=0.724 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.5, logisticregression__solver=newton-cg;, score=0.701 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.5, logisticregression__solver=newton-cg;, score=0.704 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.5, logisticregression__solver=newton-cg;, score=0.712 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.5, logisticregression__solver=newton-cg;, score=0.691 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.5, logisticregression__solver=newton-cg;, score=0.725 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.5, logisticregression__solver=lbfgs;, score=0.701 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.5, logisticregression__solver=lbfgs;, score=0.704 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.5, logisticregression__solver=lbfgs;, score=0.713 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.5, logisticregression__solver=lbfgs;, score=0.691 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.5, logisticregression__solver=lbfgs;, score=0.725 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.5, logisticregression__solver=sag;, score=0.701 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.5, logisticregression__solver=sag;, score=0.704 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.5, logisticregression__solver=sag;, score=0.712 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.5, logisticregression__solver=sag;, score=0.691 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.5, logisticregression__solver=sag;, score=0.725 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=0.5, logisticregression__solver=saga;, score=0.701 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=0.5, logisticregression__solver=saga;, score=0.704 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=0.5, logisticregression__solver=saga;, score=0.712 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=0.5, logisticregression__solver=saga;, score=0.691 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=0.5, logisticregression__solver=saga;, score=0.725 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=1, logisticregression__solver=liblinear;, score=0.703 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=1, logisticregression__solver=liblinear;, score=0.704 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=1, logisticregression__solver=liblinear;, score=0.716 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=1, logisticregression__solver=liblinear;, score=0.690 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=1, logisticregression__solver=liblinear;, score=0.725 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=1, logisticregression__solver=newton-cg;, score=0.703 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=1, logisticregression__solver=newton-cg;, score=0.704 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=1, logisticregression__solver=newton-cg;, score=0.715 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=1, logisticregression__solver=newton-cg;, score=0.692 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=1, logisticregression__solver=newton-cg;, score=0.724 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=1, logisticregression__solver=lbfgs;, score=0.703 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=1, logisticregression__solver=lbfgs;, score=0.704 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=1, logisticregression__solver=lbfgs;, score=0.715 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=1, logisticregression__solver=lbfgs;, score=0.692 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=1, logisticregression__solver=lbfgs;, score=0.724 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=1, logisticregression__solver=sag;, score=0.703 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=1, logisticregression__solver=sag;, score=0.704 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=1, logisticregression__solver=sag;, score=0.715 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=1, logisticregression__solver=sag;, score=0.692 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=1, logisticregression__solver=sag;, score=0.724 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=1, logisticregression__solver=saga;, score=0.703 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=1, logisticregression__solver=saga;, score=0.704 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=1, logisticregression__solver=saga;, score=0.715 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=1, logisticregression__solver=saga;, score=0.692 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=1, logisticregression__solver=saga;, score=0.724 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=5, logisticregression__solver=liblinear;, score=0.705 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=5, logisticregression__solver=liblinear;, score=0.705 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=5, logisticregression__solver=liblinear;, score=0.717 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=5, logisticregression__solver=liblinear;, score=0.692 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=5, logisticregression__solver=liblinear;, score=0.726 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=5, logisticregression__solver=newton-cg;, score=0.705 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=5, logisticregression__solver=newton-cg;, score=0.705 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=5, logisticregression__solver=newton-cg;, score=0.718 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=5, logisticregression__solver=newton-cg;, score=0.692 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=5, logisticregression__solver=newton-cg;, score=0.725 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=5, logisticregression__solver=lbfgs;, score=0.705 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=5, logisticregression__solver=lbfgs;, score=0.705 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=5, logisticregression__solver=lbfgs;, score=0.718 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=5, logisticregression__solver=lbfgs;, score=0.692 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=5, logisticregression__solver=lbfgs;, score=0.725 total time=   0.0s\n",
      "[CV 1/5] END logisticregression__C=5, logisticregression__solver=sag;, score=0.705 total time=   0.0s\n",
      "[CV 2/5] END logisticregression__C=5, logisticregression__solver=sag;, score=0.705 total time=   0.0s\n",
      "[CV 3/5] END logisticregression__C=5, logisticregression__solver=sag;, score=0.718 total time=   0.0s\n",
      "[CV 4/5] END logisticregression__C=5, logisticregression__solver=sag;, score=0.692 total time=   0.0s\n",
      "[CV 5/5] END logisticregression__C=5, logisticregression__solver=sag;, score=0.725 total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END logisticregression__C=5, logisticregression__solver=saga;, score=0.705 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END logisticregression__C=5, logisticregression__solver=saga;, score=0.705 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\Ricardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5] END logisticregression__C=5, logisticregression__solver=saga;, score=0.718 total time=   0.1s\n",
      "[CV 4/5] END logisticregression__C=5, logisticregression__solver=saga;, score=0.692 total time=   0.1s\n",
      "[CV 5/5] END logisticregression__C=5, logisticregression__solver=saga;, score=0.725 total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('columntransformer',\n",
       "                                        ColumnTransformer(transformers=[('minmaxscaler',\n",
       "                                                                         MinMaxScaler(),\n",
       "                                                                         [0, 2,\n",
       "                                                                          3, 4,\n",
       "                                                                          5, 6,\n",
       "                                                                          7,\n",
       "                                                                          8]),\n",
       "                                                                        ('onehotencoder',\n",
       "                                                                         OneHotEncoder(sparse=False),\n",
       "                                                                         [1])])),\n",
       "                                       ('smote', SMOTE(random_state=10)),\n",
       "                                       ('logisticregression',\n",
       "                                        LogisticRegression(random_state=10))]),\n",
       "             param_grid={'logisticregression__C': [0.01, 0.05, 0.1, 0.5, 1, 5],\n",
       "                         'logisticregression__solver': ['liblinear',\n",
       "                                                        'newton-cg', 'lbfgs',\n",
       "                                                        'sag', 'saga']},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logisticregression__C': 5, 'logisticregression__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "print(lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.709\n"
     ]
    }
   ],
   "source": [
    "print(lr_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.7072857142857143\n",
      "Testing Data Score: 0.701\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Data Score: {lr_grid.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {lr_grid.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Predictions:   [1 0 0 0 1 1 1 0 0 0]\n",
      "First 10 Actual labels: [1, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = lr_grid.predict(X_test)\n",
    "print(f\"First 10 Predictions:   {predictions[:10]}\")\n",
    "print(f\"First 10 Actual labels: {y_test[:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prediction  Actual\n",
       "0              1       1\n",
       "1              0       0\n",
       "2              0       0\n",
       "3              0       0\n",
       "4              1       0\n",
       "...          ...     ...\n",
       "2995           0       0\n",
       "2996           0       0\n",
       "2997           0       0\n",
       "2998           0       0\n",
       "2999           0       0\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1696  693]\n",
      " [ 204  407]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71 0.29]\n",
      " [0.33 0.67]]\n"
     ]
    }
   ],
   "source": [
    "cm = np.around(cm / cm.sum(axis=1)[:, np.newaxis], 2)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.71      0.79      2389\n",
      "           1       0.37      0.67      0.48       611\n",
      "\n",
      "    accuracy                           0.70      3000\n",
      "   macro avg       0.63      0.69      0.63      3000\n",
      "weighted avg       0.79      0.70      0.73      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = lr_grid.predict(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [1]\n",
      "Actual Labels: [1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted classes: {pred}\")\n",
    "print(f\"Actual Labels: {list(y_test[:1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/nate_logistic_regression.sav']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "filename = '../models/logistic_regression.sav'\n",
    "joblib.dump(lr_grid, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.701\n"
     ]
    }
   ],
   "source": [
    "lr_model = joblib.load(filename)\n",
    "print(lr_model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
